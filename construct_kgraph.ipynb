{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Project-Hackathons/LifeHack2024/blob/main/TerrorViz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZp_U3s-L1ra"
      },
      "source": [
        "# Automated Knowledge Graph Creation and Querying for Terrorism Reports Using LLMs\n",
        "## **Introduction**\n",
        "\n",
        "In the context of terrorism, reports and articles often contain extensive, unstructured text that is challenging to analyze and cross-reference in an automated manner. For instance, articles about a single terror incident might arrive at different times throughout the day, each with varying details. These reports typically include crucial entities such as *persons, objects, locations, and events*.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **Goals**\n",
        "\n",
        "1. Extract entities from these reports and represent them in a structured knowledge graph.\n",
        "2. Develop a chatbot capable of answering questions based on the generated knowledge graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP-YZNpqVHXj"
      },
      "source": [
        "# Our Solution\n",
        "In an age where timely and accurate information is crucial, our solution revolutionizes how you handle the influx of articles related to terror events. Our cutting-edge system streamlines and enhances the process of extracting critical insights from a deluge of incoming reports.\n",
        "\n",
        "First, we perform **coreference resolution** to clarify contexts and references within the articles. Then, our advanced Large Language Model (LLM) extracts specific entities and their relationships (**entity/ relationship disambiguation**), integrating them into a comprehensive **knowledge graph**. Articles are intelligently chunked, with entities **linked to their respective extracts** for precise, context-aware tracking.\n",
        "\n",
        "For retrieval, we deploy a **Multi-agent LLM system** designed to excel in the high-stakes environment of terror event reporting. This system resolves complex relationships, **identifies key entities**, generates detailed **follow-up questions**, retrieves pertinent **extracts**, and provides concise **summaries**. Additionally, it features a **decisor** agent to make informed, real-time decisions based on the aggregated data.\n",
        "\n",
        "Our solution transforms the overwhelming influx of terror-related articles into clear, actionable intelligence, empowering security professionals and decision-makers to respond swiftly and effectively. Stay informed, stay prepared, and make critical decisions with confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPkDyBARM8yr"
      },
      "source": [
        "## Dependencies\n",
        "To run this project, the following dependencies are required:\n",
        "\n",
        "*   langchain: A library to facilitate the creation of language models\n",
        "*   neo4j: A graph database management system to store and query the knowledge graph.\n",
        "*   openai: To access and use OpenAI's language models.\n",
        "*   langchain_openai: Integrates LangChain with OpenAI's models.\n",
        "*   langchain-community: Additional LangChain community tools and integrations.\n",
        "*   spacy: SpaCy is a robust Python library for advanced natural language processing tasks\n",
        "*   fastcoreref: fast, efficient library for coreference resolution that uses neural networks to identify and link references to the same entities within a text.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aHbRmW4KhiJb",
        "outputId": "1abc86b5-9bd7-47ea-af95-0c801f291d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (0.2.1)\n",
            "Requirement already satisfied: neo4j in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (5.20.0)\n",
            "Requirement already satisfied: openai in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (1.30.5)\n",
            "Requirement already satisfied: langchain_openai in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (0.1.8)\n",
            "Requirement already satisfied: langchain-community in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (0.2.1)\n",
            "Requirement already satisfied: setuptools in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (69.5.1)\n",
            "Requirement already satisfied: wheel in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (0.43.0)\n",
            "Requirement already satisfied: spacy in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (3.7.4)\n",
            "Collecting fastcoref\n",
            "  Using cached fastcoref-2.1.6.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langchain) (0.2.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langchain) (0.1.63)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langchain) (2.7.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: pytz in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from neo4j) (2024.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from openai) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langchain-community) (0.6.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: jinja2 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from spacy) (3.4.0)\n",
            "Collecting scipy>=1.7.3 (from fastcoref)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch>=1.10.0 (from fastcoref)\n",
            "  Downloading torch-2.3.0-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
            "Collecting transformers>=4.11.3 (from fastcoref)\n",
            "  Using cached transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting datasets>=2.5.2 (from fastcoref)\n",
            "  Using cached datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Collecting filelock (from datasets>=2.5.2->fastcoref)\n",
            "  Using cached filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting pyarrow>=12.0.0 (from datasets>=2.5.2->fastcoref)\n",
            "  Downloading pyarrow-16.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.5.2->fastcoref)\n",
            "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.5.2->fastcoref)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets>=2.5.2->fastcoref)\n",
            "  Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
            "Collecting xxhash (from datasets>=2.5.2->fastcoref)\n",
            "  Downloading xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets>=2.5.2->fastcoref)\n",
            "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.5.2->fastcoref)\n",
            "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets>=2.5.2->fastcoref)\n",
            "  Using cached huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: certifi in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
            "Requirement already satisfied: language-data>=1.2 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (2.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Collecting sympy (from torch>=1.10.0->fastcoref)\n",
            "  Using cached sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch>=1.10.0->fastcoref)\n",
            "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.11.3->fastcoref)\n",
            "  Downloading tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers>=4.11.3->fastcoref)\n",
            "  Downloading safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from pandas->datasets>=2.5.2->fastcoref) (2.9.0)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets>=2.5.2->fastcoref)\n",
            "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch>=1.10.0->fastcoref)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ernesttan/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.5.2->fastcoref) (1.16.0)\n",
            "Using cached datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "Downloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl (30.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.4/30.4 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.3.0-cp312-none-macosx_11_0_arm64.whl (61.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hUsing cached transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "Using cached huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
            "Downloading pyarrow-16.1.0-cp312-cp312-macosx_11_0_arm64.whl (26.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached filelock-3.14.0-py3-none-any.whl (12 kB)\n",
            "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
            "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Using cached sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
            "Downloading xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "Building wheels for collected packages: fastcoref\n",
            "  Building wheel for fastcoref (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for fastcoref: filename=fastcoref-2.1.6-py3-none-any.whl size=31253 sha256=d1e27e9ccf8127a86be899a6b8f3fd260684e73b5662e8eb04061ed857267071\n",
            "  Stored in directory: /Users/ernesttan/Library/Caches/pip/wheels/fb/6b/33/e3bba450b10a8e54d298bee91f094bff7d52193c48e987ef2d\n",
            "Successfully built fastcoref\n",
            "Installing collected packages: mpmath, xxhash, tzdata, sympy, scipy, safetensors, pyarrow-hotfix, pyarrow, networkx, fsspec, filelock, dill, torch, pandas, multiprocess, huggingface-hub, tokenizers, transformers, datasets, fastcoref\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 fastcoref-2.1.6 filelock-3.14.0 fsspec-2024.3.1 huggingface-hub-0.23.2 mpmath-1.3.0 multiprocess-0.70.16 networkx-3.3 pandas-2.2.2 pyarrow-16.1.0 pyarrow-hotfix-0.6 safetensors-0.4.3 scipy-1.13.1 sympy-1.12.1 tokenizers-0.19.1 torch-2.3.0 transformers-4.41.2 tzdata-2024.1 xxhash-3.4.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain neo4j openai langchain_openai langchain-community setuptools wheel spacy fastcoref"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeY1BRfIptQm"
      },
      "source": [
        "## Loading environment variables \n",
        "Four key environment variables are needed for this project:\n",
        "* NEO4J_URI\n",
        "* NEO4J_USERNAME\n",
        "* NEO4J_PASSWORD\n",
        "* OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jwUzgb28mdw8"
      },
      "outputs": [],
      "source": [
        "# Import secrets and initialise Neo4jGraph\n",
        "from langchain.graphs import Neo4jGraph\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "graph = Neo4jGraph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maqbU8MdVEAm"
      },
      "source": [
        "## Redefining Classes and Functions\n",
        "In this section below, we will redefine some classes and functions to fit out use case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# functions for coreference resolution\n",
        "def get_fast_cluster_spans(doc, clusters):\n",
        "    fast_clusters = []\n",
        "    for cluster in clusters:\n",
        "        new_group = []\n",
        "        for tuple in cluster:\n",
        "            (start, end) = tuple\n",
        "            span = doc.char_span(start, end)\n",
        "            new_group.append([span.start, span.end-1])\n",
        "        fast_clusters.append(new_group)\n",
        "    return fast_clusters\n",
        "\n",
        "def get_fastcoref_clusters(doc, text):\n",
        "    preds = model.predict(texts=[text])\n",
        "    fast_clusters = preds[0].get_clusters(as_strings=False)\n",
        "    fast_cluster_spans = get_fast_cluster_spans(doc, fast_clusters)\n",
        "    return fast_cluster_spans\n",
        "\n",
        "\n",
        "def core_logic_part(document: Doc, coref: List[int], resolved: List[str], mention_span: Span):\n",
        "    final_token = document[coref[1]]\n",
        "    if final_token.tag_ in [\"PRP$\", \"POS\"]:\n",
        "        resolved[coref[0]] = mention_span.text + \"'s\" + final_token.whitespace_\n",
        "    else:\n",
        "        resolved[coref[0]] = mention_span.text + final_token.whitespace_\n",
        "    for i in range(coref[0] + 1, coref[1] + 1):\n",
        "        resolved[i] = \"\"\n",
        "    return resolved\n",
        "\n",
        "def get_span_noun_indices(doc: Doc, cluster: List[List[int]]) -> List[int]:\n",
        "    spans = [doc[span[0]:span[1]+1] for span in cluster]\n",
        "    spans_pos = [[token.pos_ for token in span] for span in spans]\n",
        "    span_noun_indices = [i for i, span_pos in enumerate(spans_pos)\n",
        "        if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]\n",
        "    return span_noun_indices\n",
        "\n",
        "def get_cluster_head(doc: Doc, cluster: List[List[int]], noun_indices: List[int]):\n",
        "    head_idx = noun_indices[0]\n",
        "    head_start, head_end = cluster[head_idx]\n",
        "    head_span = doc[head_start:head_end+1]\n",
        "    return head_span, [head_start, head_end]\n",
        "\n",
        "def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):\n",
        "    return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])\n",
        "\n",
        "def improved_replace_corefs(document, clusters):\n",
        "    resolved = list(tok.text_with_ws for tok in document)\n",
        "    all_spans = [span for cluster in clusters for span in cluster] \n",
        "\n",
        "    for cluster in clusters:\n",
        "        noun_indices = get_span_noun_indices(document, cluster)\n",
        "\n",
        "        if noun_indices:\n",
        "            mention_span, mention = get_cluster_head(document, cluster, noun_indices)\n",
        "\n",
        "            for coref in cluster:\n",
        "                if coref != mention and not is_containing_other_spans(coref, all_spans):\n",
        "                    core_logic_part(document, coref, resolved, mention_span)\n",
        "\n",
        "    return \"\".join(resolved)\n",
        "\n",
        "def load_text_to_document(file_path: str) -> Document:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text_content = file.read()\n",
        "    print(f'Before Coreference Resolution:\\n{text_content}')\n",
        "    doc = nlp(text_content) \n",
        "    clusters = get_fastcoref_clusters(doc, text_content) \n",
        "    coref_text = improved_replace_corefs(doc, clusters) \n",
        "\n",
        "    document = Document(page_content=coref_text)\n",
        "    return document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aCQQjZnftvgB"
      },
      "outputs": [],
      "source": [
        "# Redefining classes to overight existing pydantic classes. \n",
        "# Final KnowledgeGraph class will be passed to LLM so that it knows the output format. \n",
        "# This is to help LLM identify Nodes and Relationships.\n",
        "\n",
        "from langchain_community.graphs.graph_document import (\n",
        "    Node as BaseNode,\n",
        "    Relationship as BaseRelationship,\n",
        "    GraphDocument,\n",
        ")\n",
        "from langchain.schema import Document\n",
        "from typing import List, Dict, Any, Optional\n",
        "from langchain.pydantic_v1 import Field, BaseModel\n",
        "\n",
        "class Property(BaseModel):\n",
        "  \"\"\"A single property consisting of key and value\"\"\"\n",
        "  key: str = Field(..., description=\"key\")\n",
        "  value: str = Field(..., description=\"value\")\n",
        "\n",
        "class Node(BaseNode):\n",
        "    properties: Optional[List[Property]] = Field(\n",
        "        None, description=\"List of node properties\")\n",
        "\n",
        "class Relationship(BaseRelationship):\n",
        "    properties: Optional[List[Property]] = Field(\n",
        "        None, description=\"List of relationship properties\"\n",
        "    )\n",
        "\n",
        "class KnowledgeGraph(BaseModel):\n",
        "    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n",
        "    nodes: List[Node] = Field(\n",
        "        ..., description=\"List of nodes in the knowledge graph\")\n",
        "    rels: List[Relationship] = Field(\n",
        "        ..., description=\"List of relationships in the knowledge graph\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "F1AJ6q_ivYGQ"
      },
      "outputs": [],
      "source": [
        "# Function that instructs LLM to identify Nodes and Relationship and output the result in the desired format (KnowledgeGraph Class)\n",
        "# Note that even though we are prompting LLM to do coreference resolution, it will be done before the document is passed into the LLM using spacy. \n",
        "# This additional coreference resolution is just an additional safety net.\n",
        "\n",
        "from langchain.chains.openai_functions import (create_structured_output_chain,)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0 )\n",
        "\n",
        "def get_extraction_chain(\n",
        "    allowed_nodes: Optional[List[str]] = None,\n",
        "    allowed_rels: Optional[List[str]] = None\n",
        "    ):\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [(\n",
        "          \"system\",\n",
        "          f\"\"\"# Knowledge Graph Instructions for GPT-4\n",
        "## 1. Overview\n",
        "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n",
        "- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
        "- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
        "## 2. Labeling Nodes\n",
        "- **Consistency**: Ensure you use basic or elementary types for node labels.\n",
        "  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
        "- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
        "{'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n",
        "## 3. Labelling Relationships\n",
        "{'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n",
        "## 4. Handling Numerical Data and Dates\n",
        "- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
        "- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
        "- **Property Format**: Properties must be in a key-value format.\n",
        "- **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
        "- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
        "## 5. Coreference Resolution\n",
        "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
        "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\n",
        "always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
        "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
        "## 6. Strict Compliance\n",
        "Adhere to the rules strictly. Non-compliance will result in termination.\n",
        "          \"\"\"),\n",
        "            (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
        "            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
        "        ])\n",
        "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "63OZD7fkukHM"
      },
      "outputs": [],
      "source": [
        "# Functions to reformat ouput by LLM before passing information over to Neo4j\n",
        "def format_property_key(s: str) -> str:\n",
        "    words = s.split()\n",
        "    if not words:\n",
        "        return s\n",
        "    first_word = words[0].lower()\n",
        "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
        "    return \"\".join([first_word] + capitalized_words)\n",
        "\n",
        "def props_to_dict(props) -> dict:\n",
        "    \"\"\"Convert properties to a dictionary.\"\"\"\n",
        "    properties = {}\n",
        "    if not props:\n",
        "      return properties\n",
        "    for p in props:\n",
        "        properties[format_property_key(p.key)] = p.value\n",
        "    return properties\n",
        "\n",
        "def map_to_base_node(node: Node) -> BaseNode:\n",
        "    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n",
        "    properties = props_to_dict(node.properties) if node.properties else {}\n",
        "    # Add name property for better Cypher statement generation\n",
        "    properties[\"name\"] = node.id.title()\n",
        "    return BaseNode(\n",
        "        id=node.id.title(), type=node.type.capitalize(), properties=properties\n",
        "    )\n",
        "\n",
        "\n",
        "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
        "    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n",
        "    source = map_to_base_node(rel.source)\n",
        "    target = map_to_base_node(rel.target)\n",
        "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
        "    return BaseRelationship(\n",
        "        source=source, target=target, type=rel.type, properties=properties\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQnisioc1d9E"
      },
      "source": [
        "## Evaluation\n",
        "Let's test out the functions we have implemented so far!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ernesttan/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "06/01/2024 09:57:29 - INFO - \t missing_keys: []\n",
            "06/01/2024 09:57:29 - INFO - \t unexpected_keys: []\n",
            "06/01/2024 09:57:29 - INFO - \t mismatched_keys: []\n",
            "06/01/2024 09:57:29 - INFO - \t error_msgs: []\n",
            "06/01/2024 09:57:29 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
          ]
        }
      ],
      "source": [
        "# instantiate nlp and model objects for coreference resolution\n",
        "import spacy\n",
        "from spacy.tokens import Doc, Span\n",
        "from fastcoref import FCoref\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "model = FCoref()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2fVOebv657Am"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06/01/2024 09:59:35 - INFO - \t Tokenize 1 inputs...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before Coreference Resolution:\n",
            "KUALA LUMPUR, Malaysia (AP) — The man who attacked a Malaysian police station and killed two officers was a recluse and is believed to have acted on his own despite suspected links to the Jemaah Islamiyah extremist group, the country’s home minister said Saturday.\n",
            "\n",
            "The man stormed the police station in southern Johor state near Singapore in the early hours of Friday with a machete. He hacked a police constable to death and then used the officer’s weapon to kill another. He wounded a third officer before being shot dead. Police initially said the man could have attempted to take firearms from the station.\n",
            "\n",
            "Home Minister Saifuddin Nasution called it a “lone wolf attack” based on an initial investigation and said there was no threat to the wider public.\n",
            "\n",
            "“We have established that the attacker acted on his own ... a lone wolf driven by certain motivation and his own understanding,” Saifuddin said. “His action is not linked to any larger mission.”\n",
            "\n",
            "Police have said the man’s father was a known member of Jemaah Islamiyah, a Southeast Asian network linked to al-Qaida, and that they found materials linked to the group in their home. Seven people including the man’s parents and three siblings were detained and police said they were searching for some 20 Jemaah Islamiyah members in the state.\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18c0dfc0d2154c20a90b7e2b851643fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06/01/2024 09:59:35 - INFO - \t ***** Running Inference on 1 texts *****\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84ed876668d74c2492dace98279b7a21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After Coreference Resolution:\n",
            "KUALA LUMPUR, Malaysia (AP) — The man who attacked a Malaysian police station and killed two officers was a recluse and is believed to have His action on The man who attacked a Malaysian police station and killed two officers's own despite suspected links to the Jemaah Islamiyah extremist group, Malaysia's home minister said Saturday.\n",
            "\n",
            "The man who attacked a Malaysian police station and killed two officers stormed the police station in southern Johor state near Singapore in the early hours of Friday with a machete. The man who attacked a Malaysian police station and killed two officers hacked a police constable to death and then used a police constable's weapon to kill another. The man who attacked a Malaysian police station and killed two officers wounded a third officer before being shot dead. Police initially said The man who attacked a Malaysian police station and killed two officers could have attempted to take firearms from a Malaysian police station.\n",
            "\n",
            "Home Minister Saifuddin Nasution called it a “lone wolf attack” based on an initial investigation and said there was no threat to the wider public.\n",
            "\n",
            "“We have established that The man who attacked a Malaysian police station and killed two officers acted on The man who attacked a Malaysian police station and killed two officers's own ... a lone wolf driven by certain motivation and The man who attacked a Malaysian police station and killed two officers's own understanding,” the country’s home minister said. “The man who attacked a Malaysian police station and killed two officers's action is not linked to any larger mission.”\n",
            "\n",
            "Police have said The man who attacked a Malaysian police station and killed two officers's father was a known member of the Jemaah Islamiyah extremist group, and that they found materials linked to the Jemaah Islamiyah extremist group in their home. Seven people including The man who attacked a Malaysian police station and killed two officers's parents and three siblings were detained and police said police were searching for some 20 the Jemaah Islamiyah extremist group members in southern Johor state near Singapore.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#coreference resolution\n",
        "document = load_text_to_document(\"./test_text.txt\")\n",
        "print(f'After Coreference Resolution:\\n{document.page_content}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    All pronouns have been replaced with an Entity. Coreference resolution has been successfully carried out! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0nljR256Rvv",
        "outputId": "bb80db27-62c7-4bc0-9559-311f1e643999"
      },
      "outputs": [],
      "source": [
        "# ONLY USE TO DELETE THE DATABASE WHEN NEEDED FOR TESTING\n",
        "# graph.query(\"MATCH (n) DETACH DELETE n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoP5bOcA1iEg",
        "outputId": "04e672e5-f1a9-43ce-bc23-1d3ec739e986"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06/01/2024 10:11:47 - INFO - \t HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "# After coreference resolution is completed, we now invoke a function for LLM to identify Nodes and Relationships\n",
        "# Nodes and Relationships are then reformatted before being passed to Neo4j to be populated\n",
        "from typing import List\n",
        "\n",
        "def extract_and_store_graph( document: str, nodes: List[str], rels: List[str] ):\n",
        "    # Extract graph data using OpenAI functions\n",
        "    extract_chain = get_extraction_chain(nodes, rels)\n",
        "    data = extract_chain.invoke(document)['function']\n",
        "    # Construct a graph document\n",
        "    graph_document = GraphDocument(nodes = [map_to_base_node(node) for node in data.nodes], relationships = [map_to_base_relationship(rel) for rel in data.rels],source = document)\n",
        "    # Store information into a graph\n",
        "    graph.add_graph_documents([graph_document])\n",
        "    return graph_document\n",
        "\n",
        "graph_doc = extract_and_store_graph(document=document,\n",
        "                                    nodes=[\"Person\", \"Object\", \"Location\", \"Event\"], \n",
        "                                    rels=[\"INVOLVED_IN\", \"ORGANIZED_BY\", \"POSSESSED_BY\", \"VICTIM_OF\", \"AFFECTED_BY\", \"USED_BY\", \"LOCATED_AT\", \"FOUND_AT\"] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hooray! The Knowledge Graph has been successfully populated. You can expect the knowledge graph to look like this: \n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"./knowledge_graph.png\" alt=\"Knowledge Graph\" width=\"500\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's identify the entities in the extract and link the nodes back to the extract. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to identify if an entity is mentioned in a extract\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0 )\n",
        "\n",
        "def get_mentions_chain(\n",
        "    node_ids: Optional[List[str]] = None\n",
        "    ):\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [(\n",
        "          \"system\",\n",
        "          f\"\"\"# Knowledge Graph Instructions for GPT-4\n",
        "## 1. Overview\n",
        "You are a top-tier algorithm to find out which entities are mentioned in an extract.\n",
        "## 2. {'- **Entities:**' + \", \".join(node_ids) if node_ids else \"\"}\n",
        "## 3. You need to output in a JSON format. It should be a list containing the entities mentioned.\n",
        "## 4. Strict Compliance\n",
        "Adhere to the rules strictly. Non-compliance will result in termination.\n",
        "          \"\"\"),\n",
        "            (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
        "            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
        "        ])\n",
        "    return create_structured_output_chain(List[str], llm, prompt, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "node_ids = [node.id for node in graph_doc.nodes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06/01/2024 10:31:03 - INFO - \t HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "06/01/2024 10:31:03 - INFO - \t HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "06/01/2024 10:31:05 - INFO - \t HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "06/01/2024 10:31:06 - INFO - \t HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "06/01/2024 10:31:07 - INFO - \t HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "06/01/2024 10:31:07 - INFO - \t HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "06/01/2024 10:31:10 - INFO - \t HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "06/01/2024 10:31:10 - INFO - \t HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "06/01/2024 10:31:12 - INFO - \t HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "06/01/2024 10:31:12 - INFO - \t HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "# chunk the extracts \n",
        "# create vector embedding\n",
        "# map the existing nodes to the extracts\n",
        "import re\n",
        "\n",
        "def combine_with_overlap(arr, overlap):\n",
        "    combined_list = []\n",
        "    length = len(arr)\n",
        "    if length < overlap:\n",
        "        return combined_list\n",
        "    i = 0\n",
        "    while i + overlap < length:\n",
        "        combined_string = ''.join(arr[i:i + overlap + 2])\n",
        "        combined_list.append(combined_string)\n",
        "        i += overlap\n",
        "    \n",
        "    return combined_list\n",
        "\n",
        "sentences = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s').split(document.page_content)\n",
        "overlapping_extracts = combine_with_overlap(sentences, 2)\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "embedding_dimension = 1536\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "for extract in overlapping_extracts:\n",
        "    def extract_entities( document: str, node_ids: List[str] ):\n",
        "        # Extract graph data using OpenAI functions\n",
        "        extract_chain = get_mentions_chain(node_ids)\n",
        "        data = extract_chain.invoke(document)['function']\n",
        "        extract_id = str(hash(extract))\n",
        "        params = {\n",
        "            \"extract_text\": extract,\n",
        "            \"embedding\": embeddings.embed_query(extract),\n",
        "            \"extract_id\": extract_id\n",
        "        }\n",
        "\n",
        "        graph.query(\n",
        "            \"\"\"\n",
        "            MERGE (n:Extract {extract_text: $extract_text, embedding: $embedding, id: $extract_id})\n",
        "            WITH n\n",
        "            CALL db.create.setVectorProperty(n, 'extract_embedding', $embedding)\n",
        "            YIELD node\n",
        "            RETURN count(*)\n",
        "            \"\"\",\n",
        "            params,\n",
        "            )\n",
        "        \n",
        "        try:\n",
        "            graph.query(\n",
        "                \"CALL db.index.vector.createNodeIndex('extract', \"\n",
        "                \"'Extract', 'embedding', $dimension, 'cosine')\",\n",
        "                {\"dimension\": embedding_dimension},\n",
        "            )\n",
        "        except:  # already exists\n",
        "            pass\n",
        "        \n",
        "        for id in node_ids:\n",
        "            graph.query(\n",
        "                \"\"\"\n",
        "                MATCH (p {id: $id})\n",
        "                MATCH (n:Extract {id: $extract_id})\n",
        "                MERGE (n)<-[:MENTIONED_IN]-(p)\n",
        "                RETURN count(*)\n",
        "                \"\"\",\n",
        "                {\"id\": id, \"extract_id\": extract_id},\n",
        "                )\n",
        "\n",
        "        # # Construct a graph document\n",
        "        # graph_document = GraphDocument(nodes = [map_to_base_node(node) for node in data.nodes], relationships = [map_to_base_relationship(rel) for rel in data.rels],source = document)\n",
        "        # # Store information into a graph\n",
        "        # graph.add_graph_documents([graph_document])\n",
        "        # return graph_document\n",
        "    graph_doc = extract_entities(document=Document(page_content=extract), node_ids=node_ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Awesome! You have successfully linked the nodes to the extract. Your Knowledge Graph should now look like this: \n",
        "<p align=\"center\">\n",
        "<img src=\"./knowledge_graph_wextracts.png\" alt=\"Knowledge Graph With Extract\" width=\"500\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### move on to [retrival.ipynb](https://github.com/Project-Hackathons/LifeHack2024/blob/main/retrival.ipynb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
