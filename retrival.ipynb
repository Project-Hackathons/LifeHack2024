{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIVlzOC8tK1T",
        "outputId": "283f2dd3-81ab-413d-96a7-79fa514434eb"
      },
      "outputs": [],
      "source": [
        "%pip install python-dotenv neo4j openai langchain langchain_openai langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "here, we are importing the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wBhqt9putH9v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "from langchain.graphs import Neo4jGraph\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from openai import OpenAI\n",
        "import ast\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "import os\n",
        "from langchain.graphs import Neo4jGraph\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.pydantic_v1 import BaseModel\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Neo4jVector\n",
        "\n",
        "graph = Neo4jGraph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we first define the prompts we will be using to \n",
        "1. identify entities mentioned in extracts\n",
        "2. generate in-depth questions from the orginal prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "EfWG7cCDukPB"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "class StringList(BaseModel):\n",
        "    strings_list: List[str]\n",
        "\n",
        "identifyEntitiesPrompt = (\n",
        "    '''\n",
        "You are given a list of entities and a question. Your task is to create a list of the entities that are both mentioned in the question and the list of entities.\n",
        "\n",
        "**Task**:\n",
        "Based on the given question, identify the nodes that already exist in our knowledge graph.\n",
        "\n",
        "**List of entities**:\n",
        "{knowledge_graph_nodes}\n",
        "\n",
        "**Instructions**:\n",
        "1. Your **output should be a list of strings**.\n",
        "2. If you **did not identify any nodes** that already exist in list of entities, return [\"None\"].\n",
        "3. If you find something similar in both the question and list of entity e.g. \"Mary Tan\" and \"Mary\", use the result from the list of entities\n",
        "4. Do no return anything not found in the List of entities\n",
        "5. **Non-compliance with these instructions will result in termination**.\n",
        "\n",
        "**Example Output**:\n",
        "- Given the question \"Bob has a car?\", the output should be:\n",
        "[\"Bob\"]\n",
        "  - if \"Bob\" is present in the List of Nodes but \"Car\" is not present.\n",
        "\n",
        "- Given the question \"Who is PM Lee?\", the output should be:\n",
        "[\"Prime Minister Lee\"]\n",
        "  - if \"Prime Minister Lee\" is present in the List of Nodes because \"PM\" is an abbreviation for \"Prime Minister\".\n",
        "\n",
        "- Given the question \"Who is Kenneth Gao?\", the output should be:\n",
        "[\"Kenneth\"]\n",
        "  - if \"Kenneth\" is present in the List of Nodes because \"Kenneth Gao\" from the question may refer to \"Kenneth\" in the List of Nodes.\n",
        "\n",
        "- Given the question \"Who is Gao?\", the output should be:\n",
        "[\"None\"]\n",
        "  - if \"Gao\" is not present in the List of Nodes\n",
        "\n",
        "**Take note** to always use values from the List of Nodes.\n",
        "    '''\n",
        ")\n",
        "\n",
        "generateQuestionsPrompt = (\n",
        "    '''\n",
        "    You are an expert machine learning engineer building an algorithm to answer\n",
        "    questions using a knowledge graph.\n",
        "\n",
        "    **Task**:\n",
        "    In the prompt that follow, you will be given a node. Your task is to\n",
        "    generate an additional question related to the initial question:\n",
        "    {init_question}.\n",
        "\n",
        "    This additional questions should help contextualise the initial\n",
        "    question in relation to the given node and be useful when searching the\n",
        "    knowledge graph later on.\n",
        "\n",
        "    **Instructions**:\n",
        "    1. Use the given node to generate questions that provide more context or\n",
        "    insight about the initial question.\n",
        "    2. Aim to cover various aspects related to the node, such as identity,\n",
        "    activities, whereabouts, preferences, etc.\n",
        "    3. The output should be a question with no additional formating, title etc\n",
        "    4. Non-compliance to the instruction will result to termination\n",
        "\n",
        "    **Example**:\n",
        "    Given the node \"Bob\" and the initial question \"Is Bob safe?\", the output\n",
        "    should be:\n",
        "    Where is Bob?\n",
        "    '''\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we then define the prompt that we will be using. here, we answer the question: \"Why was the man shot dead?\"\n",
        "\n",
        "we begin by identifying the relevant nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzhBMb3jxrKq",
        "outputId": "9158a876-3c0b-49a1-a961-094461662ef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Kuala Lumpur', 'Action', 'Man', 'Malaysian Police Station', 'Officers', 'Recluse', 'Malaysia', 'Ap', 'Jemaah Islamiyah Extremist Group', 'Home Minister', 'Saifuddin Nasution', 'Lone Wolf Attack', 'Investigation', 'Wider Public', 'Motivation', 'Understanding', 'Larger Mission', 'Father', 'Materials', 'Parents', 'Siblings', 'Detained', 'Searching', 'Saturday', 'Johor State', 'Singapore', 'Hours', 'Machete', 'Police Constable', 'Weapon', 'Third Officer', 'Shot Dead', 'Firearms']\n",
            "['Home Minister']\n",
            "What actions did the Home Minister take following the bombing incident?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Why was the man shot dead?\"\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "#retrieve the list of all the nodes\n",
        "list_nodes_raw = graph.query(\n",
        "    '''\n",
        "    MATCH (n)\n",
        "    WHERE n.name IS NOT NULL\n",
        "    RETURN n.name\n",
        "    ''')\n",
        "\n",
        "list_nodes = [ dict['n.name'] for dict in list_nodes_raw]\n",
        "identifyEntitiesPrompt = PromptTemplate.from_template(identifyEntitiesPrompt).format(knowledge_graph_nodes=list_nodes)\n",
        "\n",
        "# Identify nodes from question\n",
        "response = client.chat.completions.create(\n",
        "   messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": identifyEntitiesPrompt,\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        ")\n",
        "\n",
        "# Print the nodes in the prompt\n",
        "res = response.choices[0].message.content\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "then, we generate questions relevant to each node.\n",
        "eg. \"shot dead\" -> \"who did the shooting?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nodes = ast.literal_eval(res)\n",
        "queries = []\n",
        "\n",
        "for node in nodes:\n",
        "    #append initial question to prompt to give context\n",
        "    if node != 'None':\n",
        "        generateQuestionPrompt = PromptTemplate.from_template(generateQuestionsPrompt).format(init_question=prompt)\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "        messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": generateQuestionPrompt,\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": node,\n",
        "                }\n",
        "            ],\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "        )\n",
        "\n",
        "        query = response.choices[0].message.content\n",
        "\n",
        "    queries.append(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we now define the functions required for querying the extracts relevant to a specific node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_entity(entity_name, question):\n",
        "    entity_query = \"\"\"\n",
        "        MATCH (p)-[:MENTIONED_IN]->(e: Extract)\n",
        "        WHERE p.name = '\"\"\" + entity_name + \"\"\"' \n",
        "        WITH e, max(score) AS score \n",
        "        RETURN e.extract_text AS text, score, {} AS metadata\n",
        "        \"\"\"\n",
        "    print(\"Searching within case:\" + entity_query)\n",
        "\n",
        "    entity_vectorstore = Neo4jVector.from_existing_index(\n",
        "        OpenAIEmbeddings(),\n",
        "        index_name=\"summary\",\n",
        "        retrieval_query=entity_query,\n",
        "    )\n",
        "\n",
        "    template = \"\"\"Answer the question based only on the following context:\n",
        "        {context}\n",
        "\n",
        "        Question: {question}\n",
        "        \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "    model = ChatOpenAI()\n",
        "\n",
        "    retriever = entity_vectorstore.as_retriever()\n",
        "\n",
        "    print(retriever.get_relevant_documents(question))\n",
        "\n",
        "    chain = (\n",
        "        RunnableParallel(\n",
        "            {\n",
        "                \"context\": itemgetter(\"question\") | retriever,\n",
        "                \"question\": itemgetter(\"question\"),\n",
        "            }\n",
        "        )\n",
        "        | prompt\n",
        "        | model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return chain.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "for demonstration purposes, we query the first identified node with the first query. \n",
        "\n",
        "for a full explanation of how the llm-based multi agent system should function, please refer to the attached video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8jzXLaG_PHR",
        "outputId": "b6d6ef20-ae15-4254-a6bb-f7b525acb38d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Searching within case:\n",
            "        MATCH (p)-[:MENTIONED_IN]->(e: Extract)\n",
            "        WHERE p.name = 'Saturday' \n",
            "        WITH e, max(score) AS score \n",
            "        RETURN e.extract_text AS text, score, {} AS metadata\n",
            "The man was shot dead because he stormed the police station in southern Johor state near Singapore with a machete, where he hacked a police constable to death and then used the officerâ€™s weapon to kill another.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "qn_ans = query_entity(nodes[0], queries[0])\n",
        "print(qn_ans)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
