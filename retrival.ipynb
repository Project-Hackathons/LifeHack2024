{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Goal**\n",
        "The goal of this document is to develop a chatbot capable of answering questions based on the following knowledge graph. \n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"./assets/knowledge_graph_wextracts.png\" alt=\"Knowledge Graph With Extract\" width=\"500\"/>\n",
        "</p>\n",
        "\n",
        "If you have not seen [construct_kgraph.ipynb](https://github.com/Project-Hackathons/LifeHack2024/blob/main/construct_kgraph.ipynb), please go back to see how a knowledge graph is being created. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIVlzOC8tK1T",
        "outputId": "283f2dd3-81ab-413d-96a7-79fa514434eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.9/site-packages (1.0.1)\n",
            "Requirement already satisfied: neo4j in ./.venv/lib/python3.9/site-packages (5.20.0)\n",
            "Requirement already satisfied: openai in ./.venv/lib/python3.9/site-packages (1.30.5)\n",
            "Requirement already satisfied: langchain in ./.venv/lib/python3.9/site-packages (0.2.1)\n",
            "Requirement already satisfied: langchain_openai in ./.venv/lib/python3.9/site-packages (0.1.8)\n",
            "Requirement already satisfied: langchain-community in ./.venv/lib/python3.9/site-packages (0.2.1)\n",
            "Requirement already satisfied: pytz in ./.venv/lib/python3.9/site-packages (from neo4j) (2024.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.9/site-packages (from openai) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.9/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.9/site-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.9/site-packages (from openai) (2.7.2)\n",
            "Requirement already satisfied: sniffio in ./.venv/lib/python3.9/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.9/site-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.9/site-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.9/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.9/site-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.9/site-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.venv/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in ./.venv/lib/python3.9/site-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.venv/lib/python3.9/site-packages (from langchain) (0.2.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.venv/lib/python3.9/site-packages (from langchain) (0.1.65)\n",
            "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.9/site-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.9/site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.9/site-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in ./.venv/lib/python3.9/site-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.9/site-packages (from langchain-community) (0.6.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in ./.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install python-dotenv neo4j openai langchain langchain_openai langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once again, we are importing the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wBhqt9putH9v"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kenf/Developer/lifehack/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "from langchain.graphs import Neo4jGraph\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from openai import OpenAI\n",
        "import ast\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "import os\n",
        "from langchain.graphs import Neo4jGraph\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.pydantic_v1 import BaseModel\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Neo4jVector\n",
        "\n",
        "graph = Neo4jGraph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we first define the prompts we will be using to \n",
        "1. identify entities mentioned in extracts\n",
        "2. generate in-depth questions from the orginal prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EfWG7cCDukPB"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "class StringList(BaseModel):\n",
        "    strings_list: List[str]\n",
        "\n",
        "identifyEntitiesPrompt = (\n",
        "    '''\n",
        "You are given a list of entities and a question. Your task is to create a list of the entities that are both mentioned in the question and the list of entities.\n",
        "\n",
        "**Task**:\n",
        "Based on the given question, identify the nodes that already exist in our knowledge graph.\n",
        "\n",
        "**List of entities**:\n",
        "{knowledge_graph_nodes}\n",
        "\n",
        "**Instructions**:\n",
        "1. Your **output should be a list of strings**.\n",
        "2. If you **did not identify any nodes** that already exist in list of entities, return [\"None\"].\n",
        "3. If you find something similar in both the question and list of entity e.g. \"Mary Tan\" and \"Mary\", use the result from the list of entities\n",
        "4. Do no return anything not found in the List of entities\n",
        "5. **Non-compliance with these instructions will result in termination**.\n",
        "\n",
        "**Example Output**:\n",
        "- Given the question \"Bob has a car?\", the output should be:\n",
        "[\"Bob\"]\n",
        "  - if \"Bob\" is present in the List of Nodes but \"Car\" is not present.\n",
        "\n",
        "- Given the question \"Who is PM Lee?\", the output should be:\n",
        "[\"Prime Minister Lee\"]\n",
        "  - if \"Prime Minister Lee\" is present in the List of Nodes because \"PM\" is an abbreviation for \"Prime Minister\".\n",
        "\n",
        "- Given the question \"Who is Kenneth Gao?\", the output should be:\n",
        "[\"Kenneth\"]\n",
        "  - if \"Kenneth\" is present in the List of Nodes because \"Kenneth Gao\" from the question may refer to \"Kenneth\" in the List of Nodes.\n",
        "\n",
        "- Given the question \"Who is Gao?\", the output should be:\n",
        "[\"None\"]\n",
        "  - if \"Gao\" is not present in the List of Nodes\n",
        "\n",
        "**Take note** to always use values from the List of Nodes.\n",
        "    '''\n",
        ")\n",
        "\n",
        "generateQuestionsPrompt = (\n",
        "    '''\n",
        "    You are an expert machine learning engineer building an algorithm to answer\n",
        "    questions using a knowledge graph.\n",
        "\n",
        "    **Task**:\n",
        "    In the prompt that follow, you will be given a node. Your task is to\n",
        "    generate an additional question related to the initial question:\n",
        "    {init_question}.\n",
        "\n",
        "    This additional questions should help contextualise the initial\n",
        "    question in relation to the given node and be useful when searching the\n",
        "    knowledge graph later on.\n",
        "\n",
        "    **Instructions**:\n",
        "    1. Use the given node to generate questions that provide more context or\n",
        "    insight about the initial question.\n",
        "    2. Aim to cover various aspects related to the node, such as identity,\n",
        "    activities, whereabouts, preferences, etc.\n",
        "    3. The output should be a question with no additional formating, title etc\n",
        "    4. Non-compliance to the instruction will result to termination\n",
        "\n",
        "    **Example**:\n",
        "    Given the node \"Bob\" and the initial question \"Is Bob safe?\", the output\n",
        "    should be:\n",
        "    Where is Bob?\n",
        "    '''\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we then define the prompt that we will be using. here, we answer the question: **\"Why was the man shot dead?\"**\n",
        "\n",
        "we begin by identifying the relevant nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzhBMb3jxrKq",
        "outputId": "9158a876-3c0b-49a1-a961-094461662ef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Man', 'Shot Dead']\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Why was the man shot dead?\"\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "#retrieve the list of all the nodes\n",
        "list_nodes_raw = graph.query(\n",
        "    '''\n",
        "    MATCH (n)\n",
        "    WHERE n.name IS NOT NULL\n",
        "    RETURN n.name\n",
        "    ''')\n",
        "\n",
        "list_nodes = [ dict['n.name'] for dict in list_nodes_raw]\n",
        "identifyEntitiesPrompt = PromptTemplate.from_template(identifyEntitiesPrompt).format(knowledge_graph_nodes=list_nodes)\n",
        "\n",
        "# Identify nodes from question\n",
        "response = client.chat.completions.create(\n",
        "   messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": identifyEntitiesPrompt,\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        ")\n",
        "\n",
        "# Print the nodes in the prompt\n",
        "res = response.choices[0].message.content\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "then, we generate questions relevant to each node.\n",
        "eg. \"shot dead\" -> \"who did the shooting?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "nodes = ast.literal_eval(res)\n",
        "queries = []\n",
        "\n",
        "for node in nodes:\n",
        "    #append initial question to prompt to give context\n",
        "    if node != 'None':\n",
        "        generateQuestionPrompt = PromptTemplate.from_template(generateQuestionsPrompt).format(init_question=prompt)\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "        messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": generateQuestionPrompt,\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": node,\n",
        "                }\n",
        "            ],\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "        )\n",
        "\n",
        "        query = response.choices[0].message.content\n",
        "\n",
        "    queries.append(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we now define the functions required for querying the extracts relevant to a specific node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_entity(entity_name, question):\n",
        "    entity_query = \"\"\"\n",
        "        MATCH (p)-[:MENTIONED_IN]->(e: Extract)\n",
        "        WHERE p.name = '\"\"\" + entity_name + \"\"\"' \n",
        "        WITH e, max(score) AS score \n",
        "        RETURN e.extract_text AS text, score, {} AS metadata\n",
        "        \"\"\"\n",
        "    print(\"Searching within case:\" + entity_query)\n",
        "\n",
        "    entity_vectorstore = Neo4jVector.from_existing_index(\n",
        "        OpenAIEmbeddings(),\n",
        "        index_name=\"summary\",\n",
        "        retrieval_query=entity_query,\n",
        "    )\n",
        "\n",
        "    template = \"\"\"Answer the question based only on the following context:\n",
        "        {context}\n",
        "\n",
        "        Question: {question}\n",
        "        \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "    model = ChatOpenAI()\n",
        "\n",
        "    retriever = entity_vectorstore.as_retriever()\n",
        "\n",
        "    print(retriever.get_relevant_documents(question))\n",
        "\n",
        "    chain = (\n",
        "        RunnableParallel(\n",
        "            {\n",
        "                \"context\": itemgetter(\"question\") | retriever,\n",
        "                \"question\": itemgetter(\"question\"),\n",
        "            }\n",
        "        )\n",
        "        | prompt\n",
        "        | model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return chain.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "for demonstration purposes, we query the first identified node with the first query. \n",
        "\n",
        "for a full explanation of how the llm-based multi agent system should function, please refer to the attached video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8jzXLaG_PHR",
        "outputId": "b6d6ef20-ae15-4254-a6bb-f7b525acb38d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Searching within case:\n",
            "        MATCH (p)-[:MENTIONED_IN]->(e: Extract)\n",
            "        WHERE p.name = 'Man' \n",
            "        WITH e, max(score) AS score \n",
            "        RETURN e.extract_text AS text, score, {} AS metadata\n",
            "The man was shot dead because he stormed the police station in southern Johor state with a machete, where he hacked a police constable to death and then used the officerâ€™s weapon to kill another.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "qn_ans = query_entity(nodes[0], queries[0])\n",
        "print(f\"Answer: {qn_ans}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Congratualions! \n",
        "You have came to the end of our project. Use this [Link](https://youtube.com) for an explanation on how everything works! "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
